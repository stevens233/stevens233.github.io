<!DOCTYPE html>
<html lang="zh-CN">

<!-- <style>
    .noSelect {
        -webkit-touch-callout: none; /* iOS Safari */
        -webkit-user-select: none;   /* Chrome/Safari/Opera */
        -moz-user-select: none;      /* Firefox */
        -ms-user-select: none;       /* Internet Explorer/Edge */
        user-select: none;           /* Non-prefixed version, currently supported by most modern browsers */
    }
</style> -->

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Steven的个人博客</title>
    <link rel="stylesheet" href="/css/style_article.css">
</head>
<script src="script.js"></script>
<body>
    <header>
        <h1>欢迎来到Steven的博客</h1>
        <nav>
            <ul>
                <li><a href="/index.html">首页</a></li>
                <li><a href="#">文章</a></li>
                <li><a href="#">关于我</a></li>
                <li><a href="#">联系方式</a></li>
            </ul>
        </nav>
    </header>

    <main>
        <section>
            <h2>神经网络学习总结</h2>
            <!-- 文章列表-->
            <article>
                <pre>
卷积神经网络（CNN）的构建和训练可以大致分为以下几个步骤：

1.  准备数据集：
        收集和处理数据，通常包括加载数据、标准化（例如，图像像素值缩放到0-1）、增强（如旋转、缩放图像等）和划分训练/验证/测试集。

2.  定义CNN架构：
        输入层：接受原始数据（如图像）。
        卷积层：通过卷积核提取特征。
        激活层：如ReLU，引入非线性。
        池化层：降采样，减少参数数量和计算量。
        全连接层：传统的神经网络层，用于分类或回归输出。
        输出层：根据任务类型（分类或回归）产生输出。

3.  选择编译参数：
        损失函数：如交叉熵（分类）或均方误差（回归）。
        优化器：如SGD、Adam等。
        评价指标：如准确率。

4.  训练模型：
        使用训练数据训练网络，包括前向传播和反向传播。
        通过多个周期（epochs）训练，调整权重以最小化损失函数。

5.  评估模型：
        使用验证集和测试集评估模型的性能。
        调整模型参数或结构以改善性能。

6.  模型部署（可选）：
        将训练好的模型部署到实际应用中。



反向传播（Backpropagation）是一种用于训练人工神经网络的算法，特别是在关于梯度下降优化算法的上下文中。其基本原理可以概括为以下几个步骤：

1.  前向传播：
        数据在神经网络中从输入层通过隐藏层传递到输出层。
        在每一层，数据经过加权求和和激活函数处理。

2.  计算损失：
        在网络的输出层，损失函数计算预测值和真实值之间的差距。

3.  反向传播误差：
        算法计算损失函数相对于网络参数（通常是权重和偏置）的梯度。
        这是通过应用链式法则从输出层反向到输入层逐层进行的。对每层，计算损失相对于该层权重的偏导数。
        实际上，这意味着算法计算每个输出误差是如何随着该层权重的变化而变化的，并且这个过程会继续传递到网络的更早层。

4.  更新权重和偏置：
        使用梯度下降或其他优化算法，根据计算出的梯度来更新网络的权重和偏置。
        通常，这个步骤是通过从原始权重中减去梯度乘以学习率来完成的。

5.  迭代优化：
        上述过程在训练集上多次迭代，每次迭代都会使网络预测更接近实际标签。



在神经网络中，每一层都可以被视为一个函数，这些函数层层嵌套形成了整个网络。在反向传播算法中，我们从最后一层（通常是损失函数）开始，对前面的每一层进行逐层求导，最终得到第一层的梯度。这个过程利用了链式法则，它是微积分中的一个基本原则，用于计算复合函数的导数。

具体来说，对于每一层 LiLi​，它的输出是下一层 Li+1Li+1​ 的输入。我们可以计算损失函数相对于 LiLi​ 的输出的偏导数，然后使用链式法则将这个偏导数传递回 LiLi​ 的输入，也就是 Li−1Li−1​ 的输出。这样，我们就可以逐层向后计算每层权重的梯度。

这个过程从输出层开始，一直反向传播到输入层，允许我们根据损失函数计算每个权重的梯度，从而有效地训练神经网络。



1.  卷积：使用多个过滤器对输入数据进行卷积操作，提取特征。

2.  激活函数：应用非线性激活函数（如ReLU）。

3.  池化（可选）：进行池化操作（如最大池化），以降低特征维度并减少计算。

4.  重复卷积和池化（可选）：根据需要多次重复卷积和池化操作。

5.  展平：将多维的卷积或池化输出展平成一维向量。

6.  全连接层：通过一个或多个全连接层对特征进行加权求和，可能再次应用激活函数。

7.  输出层：产生最终输出，这可能是类别概率（分类问题）或其他形式的输出。

8.  损失函数：计算网络输出和实际标签之间的损失。

9.  反向传播：通过损失函数对网络中每层的权重进行偏导数的计算。

10. 梯度下降：根据计算的梯度更新网络权重。

11. 迭代优化：重复整个过程，通过多个epoch的训练来优化网络性能。



补充几个可能的遗漏点：

1.  初始化：在训练开始前，网络中的权重需要进行初始化。权重初始化的策略（如随机初始化、Xavier初始化等）对模型的训练和收敛速度有重要影响。

2.  正则化：为了防止过拟合，可能会在CNN中使用正则化技术，如Dropout、L1/L2正则化等。

3.  批量归一化（Batch Normalization）：这是一种用于加速训练过程并提高模型稳定性的技术，通常在卷积层和激活函数之间应用。

4.  超参数调优：CNN的性能很大程度上取决于超参数的选择，如学习率、批大小、epoch数量、卷积核大小和数量等。

5.  数据增强：在训练图像处理模型时，通常会使用数据增强技术来增加数据集的多样性，如旋转、缩放、裁剪等。

6.  评估指标：除了损失函数，还会使用其他评估指标（如准确率、召回率、F1分数等）来评价模型性能。
                </pre>
            </article>
            <!-- 更多文章-->
        </section>
    </main>

    <footer>
        <p>版权所有 &copy; 2024 Steven的博客</p>
    </footer>

</body>
</html>